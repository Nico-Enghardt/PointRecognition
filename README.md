# Point-Recognition

Estimating position of the index finger from a single picture.
Vision of the project is to create a human-computer-interface which doesn't need a mouse and enables the user to interact using rich and intuitive gestures.

## Flow Chart of submodules

![Process](Images/PointRecognition-Process.png "Flow Chart of submodules")

# Dataset Creation
Training data are generated by capturing images of my hand, with the index finger position labelled automatically by a pink dot on the finger. The dot is removed in the image before training. For every image exists a 3-dimensinal label consisting of X and Y for the position of the hand in the frame and Z representing the height of the hand above the table. The X and Y positions of the red point at the index finger are recorded by a webcam mounted above, the Z coordinate is recorded by a webcam on the table.

## Experimenting with different image formats

The images are compressed to different small pixel sizes. Different compressors are used, keeping hue and grayscale or saturation values instead of rgb values.

![Example of dataset image series](Images/Hsv60-Example.mp4 "Example of dataset image series")

# Building the model

Input of the model is a single image with whatever channels (rgb/hsv...) attached. The model generates a three-dimensional continuous output (X,Y,Z). Between input and output layers are few fully connected layers with decreasing number of neurons.

## Loss function

The model should be able to predict the X-Y position on the table as well as the height of the finger in 3D-space. I created two different loss functions, therefore I could balance the cost function according to the different goals. 

## Training

First I trained the model locally, then I switched to training in Google Colab with much more computing power. I use WandB to record and analyze the training procedures as well as storing the datasets and artifacts in the WandB cloud. 